{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "057cafae-4669-46fe-a45e-24c26c1750b3",
   "metadata": {},
   "source": [
    "# Übung 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af33f65e-1fdf-40a5-a59f-5c7ba233f454",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all packages needed\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "658f13b4-d5a9-4e7d-9991-36f5961a955c",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d50d5414-c4c1-476c-bf1d-119eecc50c29",
   "metadata": {},
   "source": [
    "## 1. Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d1935d-ddde-4ab2-a93f-23c5614af190",
   "metadata": {},
   "source": [
    "Für die erste Aufgabe verwenden wir ein Datenset des [UCI Machine Learning Repositories](https://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data), welches sich um [Daten von Automobilen](https://archive.ics.uci.edu/ml/datasets/automobile) handelt.\n",
    "\n",
    "Dieses Datenset hat sowohl kategorische als auch nummerische Variablen. Die Aufbereitung und erste Sichtung der Daten ist im folgenden Code-Absatz bereits vorgenommen worden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b05deacb-e371-4567-bffc-514b7f2f5cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definiere die Spaltennamen als Liste\n",
    "headers = [\"symboling\", \"normalized_losses\", \"make\", \"fuel_type\", \"aspiration\",\n",
    "           \"num_doors\", \"body_style\", \"drive_wheels\", \"engine_location\",\n",
    "           \"wheel_base\", \"length\", \"width\", \"height\", \"curb_weight\",\n",
    "           \"engine_type\", \"num_cylinders\", \"engine_size\", \"fuel_system\",\n",
    "           \"bore\", \"stroke\", \"compression_ratio\", \"horsepower\", \"peak_rpm\",\n",
    "           \"city_mpg\", \"highway_mpg\", \"price\"]\n",
    "\n",
    "# Lese die CSV-Datei ein konvertiere die \"?\" als NaN-Werte\n",
    "auto_df = pd.read_csv(\"./src/imports-85.data\", header = None,\n",
    "                      names = headers, na_values=\"?\")\n",
    "\n",
    "# Erstsichtung der Daten\n",
    "auto_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c714f80-05a3-41a9-9dee-7e3516d53a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sichtung der Datentypen\n",
    "auto_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4888da08-e180-4540-912f-42c01c7e7d0e",
   "metadata": {},
   "source": [
    "In dieser Aufgabe werden wir uns maßgeblich mit den kategorischen Variablen beschäftigen, also den Spalten, die `object` als Datentyp im Datensatz ausweisen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa97d945-7773-4ec4-9881-bf3c892899af",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b862a4-b13f-4d67-9509-770b84580f35",
   "metadata": {},
   "source": [
    "**Aufgabe 1**: Erstelle eine Kopie des Datensatzes, welches nur die Spalten mit kategorischen Werten beinhaltet und nenne dieses `cat_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f31aeff-979f-494a-9f83-e3102541aed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4bab3d-bf31-4bba-bfd3-e94b56e73b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "check = any(item in list(auto_df.select_dtypes(include=['int', 'float']).columns) for item in list(cat_df.columns))\n",
    "\n",
    "if cat_df.shape[0] == auto_df.shape[0] and cat_df.shape[1] == 10 and not check:\n",
    "    print(\"Test successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e2373-504a-437b-9f08-324a1a821924",
   "metadata": {},
   "source": [
    "Bevor wir den Encodierung starten, müssen wir uns vorab noch mit den fehlenden Werten auseinandersetzen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d52ed1-e76a-4fa3-a487-7c2fb3b4feb0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf2568-4ce5-4a7f-a979-9387efc1b78c",
   "metadata": {},
   "source": [
    "**Aufgabe 2:** Weise die Zeilen des Datensatzes aus, die mindestens einen fehlenden Wert vorweisen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ce41f3-d018-4edb-9a23-a90d631242ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f626d5-e5ac-481c-a635-f2bc8b15176f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18b91ee-7611-471b-9e40-93915421dd9f",
   "metadata": {},
   "source": [
    "**Aufgabe 3:** Für die fehlenden Werte der Spalte `num_doors`, finde den Wert der am häufigsten vorkommt und ersetze die fehlenden Werte mit diesen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94cf93b-9ec6-4866-9026-7679a2c919eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022cd3e-dcb8-4a13-84ff-677a279abc96",
   "metadata": {},
   "outputs": [],
   "source": [
    "###  TEST HERE ###\n",
    "if cat_df[cat_df.isnull().any(axis=1)].shape[0] == 0 and cat_df.loc[[27, 63], :]['num_doors'].unique()[0] == 'four':\n",
    "    print(\"Test successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae0a58-ce01-4b78-881b-4a0f19d8da1e",
   "metadata": {},
   "source": [
    "Nach dem Abschluss der Bereinigung, widmen wir uns im nächsten Schritt der Encodierung des Datensets. Hierfür würden wir die Werte, als ausgeschriebene Zahlen im Datenset sind, in Integer überführen. In diesem Beispiel sind es `[num_doors, num_cylinders]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39169966-591c-45ab-9a44-838a29fa4cc9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7eaf9a2-a335-4457-8fb0-7a50534b84de",
   "metadata": {},
   "source": [
    "**Aufgabe 4:** Ersetze die Werte der Spalten `[num_doors, num_cylinders]` durch Integer. Finde im ersten Schritt die jeweiligen Werte heraus und ersetze diese anschließend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dcd856f-3dad-4363-ae8e-37943c1ac430",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dbab0f8-1144-4863-85d1-fa50c9ae48d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a06dcb9-692e-4d04-90e1-d129e82f2a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ####\n",
    "num_1 = [116, 89]\n",
    "num_2 = [159, 24, 11, 5, 4, 1, 1]\n",
    " \n",
    "if num_1 == list(cat_df['num_doors'].value_counts()) and num_2 == list(cat_df['num_cylinders'].value_counts()):\n",
    "    if all(list(cat_df.select_dtypes(include=[int]).columns) for item in ['num_doors', 'num_cylinders']):\n",
    "        print(\"Test successful\")\n",
    "    \n",
    "cat_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffd54b-13f9-4e75-8582-46f544194bf9",
   "metadata": {},
   "source": [
    "Nachdem wir die ersten Werte überführt haben, würde wir gerne auch ein `Label Encoding` auf eine Spalte anwenden, die nicht per se nummerische Werte beinhaltet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "180b4ffb-0f83-48bb-a7e4-897d81af5587",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e1b502-e318-40f2-907f-599b8b11b157",
   "metadata": {},
   "source": [
    "**Aufgabe 5**: Transformiere die folgenden Werte in `body_style` wie folgt:\n",
    "- convertible = 0\n",
    "- hardtop = 1\n",
    "- hatchback = 2\n",
    "- sedan = 3\n",
    "- wagon = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cafacbc-0007-4b44-b4ba-edbb79cf95eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d8e4e48-eff5-4ce3-88a9-01c35c39a589",
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST HERE ####\n",
    "style_list = ['convertible', 'hardtop', 'hatchback', 'sedan', 'wagon']\n",
    "if cat_df[\"body_style\"].sum() == 536 and all(list(le.classes_) for item in style_list):\n",
    "    print(\"Test successful\")\n",
    "    \n",
    "cat_df['body_style'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e91d3b8-c16f-4c59-ba4d-7abee0a26455",
   "metadata": {},
   "source": [
    "Nach der erfolgreichen Transformation durch Nutzung des `Label Encoders`, merken wir, dass die Transformation eine implizte Rangfolge vorgibt und dies für unsere Ziele der Datenanalyse nicht hilfreich ist. Deswegen würden wir gerne für die Spalte `drive_wheels` die Methode `One-Hot-Encoding` verwenden."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb46ab73-b360-4f3f-b410-9c21dc0cd336",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c3cb3c-145c-4cc0-928b-e779241dc058",
   "metadata": {},
   "source": [
    "**Aufgabe 6:** Transformiere die Elemente der Spalte `drive_wheels` in separate Spalten mit den Werten 0 und 1 durch Nutzung des `One-Hot-Encodings`. Nehme zum Ende die bisherige `drive_wheels` Sparte aus dem Dataset. Die neuen Spalten sollten in der Benamung den Prefix `drive_wheels_`enthalten. \n",
    "\n",
    "**Tipp:** Wenn `n` die Anzahl der Elemente der Spalte `drive_wheels` ist, sollte die Encodierung sollte in `N-1`Merkmale enthalten. Bitte schreibe auch eine kurze Begründung, warum das sinnvoll ist."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70427351-c013-42c2-bade-5b54cdbb566e",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee70e1b7-f6c7-4973-b79c-2b047a2df914",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c7aad0-8998-41af-86ce-48400959e073",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if cat_df['drive_wheels_fwd'].sum() + cat_df['drive_wheels_rwd'].sum() == 196 \\\n",
    "    and cat_df.shape[0] * cat_df.shape[1] == 2255:\n",
    "    print(\"Test successful\")\n",
    "    \n",
    "cat_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8894164f-c731-42e0-ad93-459f6d0d7e88",
   "metadata": {},
   "source": [
    "Im nächsten Schritt würden wir gerne die Spalte `engine_type` transformieren. Lasst uns zuerst einem Blick auf die Verteilung werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2bbd664-f7b1-4c53-aa9d-3b03027378e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_df['engine_type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48d1fd2f-4fac-44d9-88e8-3ab4f928e670",
   "metadata": {},
   "source": [
    "`ohc` als Kategorie der `engine_type` Spalte scheint in unterschiedlichen Ausprägungen vorzukommen. Dies würden wir gerne vereinheitlichen und anschließend alleinig die Unterscheidung machen, ob ein Motor `ohc`ist oder nicht."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94a3ea2-95b2-4daf-be7d-7055ccda2731",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2258fb3a-902b-437d-a01e-f0176cd2ed81",
   "metadata": {},
   "source": [
    "**Aufgabe 7**: Gruppiere alle `ohc` Typen und führe anschließend eine Transformation in nummerische Werte durch, sodass alle `ohc` Werte in `1` überführt werden und alle **nicht** `ohc` Werte in 0. Benenne die neue Spalte `ohc_code` und entferne anschließend die bisherige `engine_type` Spalte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904ce08-4575-489f-855a-026a98f70a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbea5bee-8e03-42af-b0c8-dfd067138037",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if cat_df['ohc_code'].sum() == 189 and cat_df.loc[1:20 ,:]['ohc_code'].sum() == 19:\n",
    "    print(\"Test successful\")\n",
    "    \n",
    "cat_df.sample(10)['ohc_code']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd35a90d-7b6b-4908-a872-7d3573711721",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64647910-9fb0-4a4f-a3af-d4067ba247a5",
   "metadata": {},
   "source": [
    "## 2. Logarithmic, Square-Root und Box-Cox Transformationen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e60d2c-7083-4d1f-a209-5c595b637c5b",
   "metadata": {},
   "source": [
    "Zur Behandlung von rechtsschiefen Verteilungen können wir Methoden wie Logarithmic Transformation verwenden. Diese Methoden helfen uns dabei, die Verteilung in eine Form zu bringen, die sich eher einer Normalverteilung annähert. Dies ist zum Beispiel wichtig für die Verwendung des ML-Algorithmus `Linear Regression`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85276ea8-f4c6-47c8-9b5d-b4c8f4d66753",
   "metadata": {},
   "source": [
    "Für die folgenden Aufgaben verwenden wir den [Boston Housing Datensatz](https://www.kaggle.com/c/boston-housing), der 506 Häuserverkäufe darlegt mit wichtigen Informationen über das Haus und deren Umgebung.\n",
    "\n",
    "- `crim`: Pro-Kopf-Verbrechensrate nach Stadt.\n",
    "- `zn`: Anteil der Wohnbauflächen, die für Grundstücke über 25.000 m² ausgewiesen sind\n",
    "- `indus`: Anteil der nicht für den Einzelhandel bestimmten Gewerbeflächen pro Stadt\n",
    "- `chas`: Charles River Dummy-Variable (= 1, wenn das Gebiet an den Fluss grenzt; sonst 0)\n",
    "- `nox`: Konzentration von Stickoxiden (Teile pro 10 Millionen)\n",
    "- `rm`: Durchschnittliche Anzahl der Zimmer pro Wohnung\n",
    "- `age`: Anteil der Eigentumswohnungen, die vor 1940 gebaut wurden\n",
    "- `dis`: Gewichtetes Mittel der Entfernungen zu fünf Bostoner Beschäftigungszentren\n",
    "- `rad`: Index der Erreichbarkeit von Radialautobahnen\n",
    "- `tax`: Vollwertiger Grundsteuersatz pro 10.000 Dollar\n",
    "- `ptratio`: Schüler-Lehrer-Verhältnis nach Stadt\n",
    "- `black`: 1000(Bk - 0.63)^2 wobei Bk der Anteil der Schwarzen in der Stadt ist\n",
    "- `lstat`: Unterer Status der Bevölkerung (Prozent)\n",
    "- `price`: Medianwert der Eigenheime in 1000er-Jahren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38de2f0e-60bf-49fb-9e13-ef293d00ad04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importiere ein Teilpackage von SKlearn zum Laden des Datensets\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "# Lade das Datenset als JSON-Format\n",
    "boston = load_boston()\n",
    "\n",
    "# Überführe den JSON-Datensatz in einen Pandas DataFrame\n",
    "boston_df = pd.DataFrame(data = boston['data'], columns = boston['feature_names'])\n",
    "boston_df['PRICE'] = boston['target']\n",
    "\n",
    "# Erhalte einen ersten Eindruck des DataFrames\n",
    "boston_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a022d2-9448-4927-95f4-da64de968560",
   "metadata": {},
   "source": [
    "Mit Hilfe der [`.skew()`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.skew.html) Funktion von Pandas erhalten wir einen ersten Eindruck der **Schiefe** der Verteilung.\n",
    "\n",
    "Wie sind die Ausgabewerte zu interpretieren:\n",
    "- Ein Schiefe-Wert von 0 bedeutet eine symmetrische Verteilung\n",
    "- Ein negativer Schiefe-Wert weist auf eine asymmetrische Verteilung hin, dessen Ende (Tail) auf der linken Seite der Verteilung größer ist - **left skewed**\n",
    "- Ein positiver Schiefe-Wert weist auf eine asymmetrische Verteilung hin, dessen Ende (Tail) auf der rechten Seite der Verteilung größer ist - **right skewed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2065225-6455-4e79-83c7-3b2199f18233",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_skewness = boston_df.skew().sort_values(ascending=False)\n",
    "\n",
    "plt.figure(figsize=(15,8))\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.barplot(x = list(boston_skewness.index), y = boston_skewness.values, \n",
    "            color = sns.color_palette()[0], alpha=.7)\n",
    "plt.ylabel('Schiefewerte')\n",
    "plt.xlabel('Variablen Boston Housing Datensatz')\n",
    "plt.title('Schiefewerte der Variablen des Boston Housing ')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c01f9b-b1ba-42aa-a4a6-01d465f31513",
   "metadata": {},
   "source": [
    "Die Darstellung gibt wieder, dass `CRIM` einen starken Schiefewert aufweißt und **right skewed** ist. Das können wir uns durch eine separate Darstellung noch einmal anders ausgeben lassen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bf97d96-ad25-4911-8437-f30cb195a012",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_crim = round(boston_df['CRIM'].max(), 1)\n",
    "text = f\"Max. Wert: {max_crim}\"\n",
    "text_kwargs = dict(ha='center', va='center')\n",
    "\n",
    "ax = plt.figure(figsize=(15,8))\n",
    "ax = sns.histplot(boston_df['CRIM'], kde = True, stat = 'density', bins = 40)\n",
    "ax.annotate(\"\", xy=(68, 0.02), xytext=(65, 0.06), xycoords='data', arrowprops=dict(arrowstyle=\"->\", color = 'red'))\n",
    "ax.text(x = 65, y = 0.07, s = text, **text_kwargs)\n",
    "\n",
    "plt.title(\"Boston Housing Datensatz - Verteilung CRIM\")\n",
    "plt.xlim(-1, 70)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598e9e1e-0889-4c1c-94df-d43efb6a811c",
   "metadata": {},
   "source": [
    "Der starke Wert aus der vorherigen Abbildung wurde durch die Abbildung der Verteilung von `CRIM` als Histogramm noch einmal bestätigt. `CRIM` folgt definitiv keiner Normalverteilung. Wichtig ist, dass wir es richtig interpretieren.\n",
    "\n",
    "**Frage**: Was zeigt das Histogramme, wenn es besonders viele niedrige Werte gibt?\n",
    "\n",
    "Ziel sollte es sein, die Skewness zu reduzieren. Als erstes würden wir dies gerne durch eine Log-Transformation durchführen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e396c37-697c-43a9-a4cf-11a260ad547d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d7e2a2-4249-45a7-8eb7-2adfab336853",
   "metadata": {},
   "source": [
    "**Aufgabe 8:** Führe eine Log-Transformation durch und berechne danach den neuen Skew-Wert für die Variable `CRIM_log` und speichere diesen unter der Variable `CRIM_log_score`. Füge die neue Verteilung dem DataFrame `boston_df` unter der Spaltenbezeichnung `CRIM_log`hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec3009f-408b-4b56-b126-4560937a14d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ca6f4-d487-4b24-aa43-f7ab7b9341f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if -395 <= boston_df['CRIM_log'].sum() <= -394 and 0.4 <= CRIM_log_score <= 0.41: \n",
    "    print(\"Test successful\")\n",
    "    \n",
    "    CRIM_log_score_2 = round(CRIM_log_score, 2)\n",
    "    text = f\"Neuer Skew Wert: {CRIM_log_score_2}\"\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,8))\n",
    "    ax = sns.histplot(boston_df['CRIM_log'], kde = True, stat = 'density', bins = 40)\n",
    "    ax.text(x = 0, y = 0.21, s = text, **text_kwargs, color = 'red')\n",
    "\n",
    "    plt.title(\"Boston Housing Datensatz - Verteilung CRIM Log\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ab96d9-5548-41d4-95b4-c13b80b100c2",
   "metadata": {},
   "source": [
    "**Frage**: Wie sind die Ergebnisse zu interpretieren?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cff9e52-2cc2-40e6-b592-a4827aa49062",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c74e30d6-381c-4a03-9387-0910a311a39d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac41104-30f1-4179-8392-3e4da4d6a8ad",
   "metadata": {},
   "source": [
    "**Aufgabe 9**: Vergleiche die Ergebnisse mit einer `log2` Transformation und interpretiere die Ergebnisse. Berechne anschließend den neuen Skew-Wert für die Variable `CRIM_log2` und speichere diesen unter der Variable `CRIM_log2_score`. Füge die neue Verteilung dem DataFrame `boston_df` unter der Spaltenbezeichnung `CRIM_log2`hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdcbafd-cb67-4a07-bdc2-ff18e4d5d702",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a319031b-4312-43f9-8deb-ba62ea23e84b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if -570 <= boston_df['CRIM_log2'].sum() <= -569 and 0.4 <= CRIM_log_score <= 0.41: \n",
    "    print(\"Test successful\")\n",
    "    \n",
    "    CRIM_log2_score_2 = round(CRIM_log2_score, 2)\n",
    "    text = f\"Neuer Skew Wert: {CRIM_log2_score_2}\"\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,8))\n",
    "    ax = sns.histplot(boston_df['CRIM_log2'], kde = True, stat = 'density', bins = 40)\n",
    "    ax.text(x = 0.15, y = 0.14, s = text, **text_kwargs, color = 'red')\n",
    "\n",
    "    plt.title(\"Boston Housing Datensatz - Verteilung CRIM Log2\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb71265-3250-4db7-9a95-bc2609a2a59a",
   "metadata": {},
   "source": [
    "**Frage:** Was ist die Interpretation der Ergebnisse? Warum sehen die Verteilungen gleich aus und es ergibt den selben **Skewness-Wert**?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f87efa4-3b3b-4d0b-83aa-36e404f89e6c",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70726cb-2256-4a33-bbb7-29d96c7b39e4",
   "metadata": {},
   "source": [
    "**Aufgabe 10:** Vergleiche die Ergebnisse mit einer `square-root` Transformation und interpretiere die Ergebnisse. Berechne anschließend den neuen Skew-Wert für die Variable `CRIM_sqr` und speichere diesen unter der Variable `CRIM_sqr_score`. Füge die neue Verteilung dem DataFrame `boston_df` unter der Spaltenbezeichnung `CRIM_log2`hinzu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "925d42b4-0893-408c-9209-8622d5b6ccec",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ####\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4830f2-5fda-44de-b726-fdad355e3a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE #####\n",
    "if 622 <= boston_df['CRIM_sqr'].sum() <= 623 and 0.2 <= CRIM_log_score <= 2.03: \n",
    "    print(\"Test successful\")\n",
    "    \n",
    "    CRIM_sqr_score_2 = round(CRIM_sqr_score, 2)\n",
    "    text = f\"Neuer Skew Wert: {CRIM_sqr_score_2}\"\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,8))\n",
    "    ax = sns.histplot(boston_df['CRIM_sqr'], kde = True, stat = 'density', bins = 40)\n",
    "    ax.text(x = 4, y = 0.84, s = text, **text_kwargs, color = 'red')\n",
    "\n",
    "    plt.title(\"Boston Housing Datensatz - Verteilung CRIM Square Root Transformation\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5efa12a-a02e-4ef9-a076-0e61c4df65b4",
   "metadata": {},
   "source": [
    "**Interpretation:** In der Vorlesung haten wir bereits gelernt, dass bei der `Square Root`-Transformation höhere Werte stärker komprimiert werden, so dass niedrigere Werte besser verteilt werden.\n",
    "\n",
    "Die Square Root Transformation ist hierbei aber nicht so aggressive wie eine Log-Transformation.\n",
    "\n",
    "Die Verteilung hier sieht nur bedingt exponential aus, sodass der `skew` Wert größer ist, als der der Log-Transformation.\n",
    "\n",
    "Der Wertebereich für `CRIM_sqr` ist auch komprimierter als der, der `log` Transformationen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75fc52c9-c28a-405f-b9db-cad7d37b7f09",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a707c-2e6e-46f2-8ef1-1e6ebee82acc",
   "metadata": {},
   "source": [
    "**Aufgabe 11 (Zusatzaufgabe)**: Vergleiche die Ergebnisse mit einer `Box-Cox` Transformation und interpretiere die Ergebnisse. Berechne anschließend den neuen Skew-Wert für die Variable `CRIM_bc` und speichere diesen unter der Variable `CRIM_bc_score`. Füge die neue Verteilung dem DataFrame boston_df unter der Spaltenbezeichnung `CRIM_bc` hinzu.\n",
    "\n",
    "Informationen zur Box-Cox-Transformationen:\n",
    "- [Intro to Box Cox Transformation](https://www.youtube.com/watch?v=pjwDpF_Igkw)\n",
    "- [Python | Box-Cox Transformation](https://www.geeksforgeeks.org/box-cox-transformation-using-python/)\n",
    "- [scipy.stats.boxcox](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.boxcox.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542628ce-b155-468b-b4e0-951e87a18326",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29681a14-f117-4c35-920c-c5a3783482f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if -556 <= boston_df['CRIM_bc'].sum() <= -555 and 0.09 <= CRIM_bc_score <= 0.1: \n",
    "    print(\"Test successful\")\n",
    "    print(\"Der optimale Wert für λ ist:\", round(stats.boxcox(boston_df['CRIM'])[1], 2))\n",
    "    \n",
    "    CRIM_bc_score_2 = round(CRIM_bc_score, 2)\n",
    "    text = f\"Neuer Skew Wert: {CRIM_bc_score_2}\"\n",
    "    \n",
    "    ax = plt.figure(figsize=(15,8))\n",
    "    ax = sns.histplot(boston_df['CRIM_bc'], kde = True, stat = 'density', bins = 40)\n",
    "    ax.text(x = -0.5, y = 0.18, s = text, **text_kwargs, color = 'red')\n",
    "\n",
    "    plt.title(\"Boston Housing Datensatz - Verteilung CRIM Box-Cox Transformation\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c7a39fd-c163-4cac-8bac-2f127b41e485",
   "metadata": {},
   "source": [
    "**Interpretation:** Der Schiefewert ist von `5.2` auf nur noch `0.09` gesunken. Die Verteilung ist der durch die Log-Transformation entstandenen ziemlich ähnlich, nur etwas weniger bimodal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba4cb33-b2ac-48c8-b981-751806401fac",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "402c249b-7ee2-4cdb-9c2a-74eef7bc73da",
   "metadata": {},
   "source": [
    "## 3. MIN-MAX, Z-Score und Robust-Scaler Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcbb35b1-bbbe-48c2-be80-a6410d2d1d1a",
   "metadata": {},
   "source": [
    "Als Nächstes würden wir gerne weitere Methoden der **Standardisierung** verwenden, um Werte auf standardisierte Wertebereiche zu bringen.\n",
    "\n",
    "Viele ML-Algorithmen performen besser mit skalierten Wertebereichen der Inputvariablen. Das inkludiert Algorithmen, die die **gewichtete Summe der Inputvariablen** verwenden, z. B. Linear Regression, Logistic Regression oder Neural Networks (Deep Learning), und Algorithmen die **Distanzen messen**, z. B. K-Nearest Neighbors oder Support Vector Machines.\n",
    "\n",
    "Mit unterschiedlichen Wertebereichen und Einheiten (z. B. KM, Stunden, ...) haben die Inputvariablen auch unterschiedliche Wertebereiche. Die größeren Wertebereiche werden hier potenziell stärker gewichtet beim direkten Einsatz eines ML-Algorithmen, sodass dies in schlechterer Performance und Insensitvität gegenüber kleineren Wertebereichen resultiert.\n",
    "\n",
    "In Allgemeinen macht es auch Sinn, wenn wir sowohl die **Input- als auch Outputvariablen** standardisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c89988-725b-4cf9-98be-666b757b3f85",
   "metadata": {},
   "source": [
    "**Datensatz:** Im nächsten Beispiel werden wir ein Datensatz verwenden für eine binäre Klassifizierung. Er umfasst 60 Eingabewerte und eine Zielvariable mit zwei Klassen. Der Datensatz enthält 208 Beispiele und die Klassen sind einigermaßen ausgewogen.\n",
    "\n",
    "Der Datensatz beschreibt Radarrückmeldungen von Felsen oder simulierten Minen und ist hier zugänglich als [**Sonar Datensatz**](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv).\n",
    "\n",
    "Eine **Beschreibung des Datensatzes** findet Ihr [hier](https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.names).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80106e2-4058-4ba8-9483-84e1bba0e559",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Laden und Erstsichtung des Datensatzes\n",
    "sonar_df = pd.read_csv('src/sonar_dataset', index_col=0)\n",
    "sonar_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b426d4-52ec-4e5d-88fe-3bef9771a734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Überprüfung des korrekten Formats\n",
    "np.array_equal(sonar_df.shape, [208, 61])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b83eea-b7e9-42c0-a54a-a5d8b88a977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zusammenfassung über alle Spalten\n",
    "sonar_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae666a99-0b32-49b3-a7a6-a041b1f3d43d",
   "metadata": {},
   "source": [
    "Die statistische Zusammenfassung zeigt, dass die bereitgestellten Eingabevariablen numerisch sind und ungefähr zwischen 0 und 1 liegen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a65838d1-98fe-4172-b09f-09f8a3d6f7d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Darstellung der Verteilungen aller Variablen\n",
    "params = {'axes.titlesize':'6',\n",
    "          'xtick.labelsize':'6',\n",
    "          'ytick.labelsize':'6'}\n",
    "\n",
    "matplotlib.rcParams.update(params)\n",
    "sonar_df.hist(bins=30, figsize=(15, 10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd404f41-f773-4e81-a3e4-b787a85e9d20",
   "metadata": {},
   "source": [
    "Wenn wir die Unübersichtlichkeit der Diagramme ignorieren und uns auf die Histogramme selbst konzentrieren, können wir erkennen, dass viele Variablen eine schiefe Verteilung aufweisen.\n",
    "\n",
    "Um es etwas interessanter für Euch zu gestalten und den Impact der jeweiligen Transformationen in der Klassifikation zu sehen, habe ich ein kleines Skript bereitgestellt, welches die transformierten Werte nimmt und eine K-Nearest Neighbour Klassifizierung als Test durchführt und das Ergebnis ausgibt.\n",
    "\n",
    "Wenn Ihr Interesse habt, könnt Ihr gerne selbst in das Skript reingucken. Das ist aber kein Muss, sondern eine interessante zusätzliche Aufgabe für diejenigen, die Lust haben.\n",
    "\n",
    "Als erstes Testen wir die Performance ohne Transformationen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa9f00-24d1-4ebe-9446-8cb26c3209ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ml_model_and_test as ml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d98c0d3-7bbe-4225-9c82-19d8e3a5ff1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "ml.evaluation(X = sonar_df.values[:, :-1], y = sonar_df.values[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbed4e5-20c6-43f7-9bd7-4479f0801966",
   "metadata": {},
   "source": [
    "Wir können sehen, dass das Model mit einer Genauigkeit von ca. 80% korrekt vorhergesagt wird, ob die Frequenzenmessungen ein metallisches oder steinartiges Objekt erkennen.\n",
    "\n",
    "**Anmerkung**: Bitte bedenke, dass auf Grundlage der Natur des Algorithmus die Ergebnisse leicht abweichen können."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0eb9882-a779-48dc-a9c4-507445402655",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe78b8b-f2db-4f93-a565-b61da87fe3b7",
   "metadata": {},
   "source": [
    "**Aufgabe 12**: Führe eine Transformation mit Hilfe der `Min-Max Skalierung` durch, um die Werte auf einen Bereich von 0 bis 1 zu standardisieren. Führe anschließend den Test durch und sehe, wie sich die Performance verändert. Sichere die transfomierten Inputvariablen in einen neuen DataFrame mit der Bezeichnung `sonar_df_mmtrans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "874263e2-f430-48f8-92a2-f26f50c68a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7e0660-47c3-4872-a16e-9075ca7b4e6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if all(sonar_df_mmtrans.describe().loc['max'].values.round(decimals=0) == 1) and \\\n",
    "    all(sonar_df_mmtrans.describe().loc['min'].values.round(decimals=0) == 0) and \\\n",
    "        sonar_df_mmtrans.sum().sum() == 4354.773996015757:\n",
    "    \n",
    "        print(\"Test successful\")\n",
    "        \n",
    "        print(sonar_df_mmtrans.describe())\n",
    "        sonar_df.hist(bins=30, figsize=(15, 10))\n",
    "        plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fd59eb-16f8-4d4c-b9fe-108651643037",
   "metadata": {},
   "source": [
    "Wir können sehen, dass die Verteilungen angepasst wurden und dass die Minimal- und Maximalwerte für jede Variable nun bei 0.0 bzw. 1.0 liegen. Darüber hinaus hat sich die Art der Verrteilungen, durch groben Vergleich der Histogramme, wenig verändert. Lasst uns nun die transformierten Werte verwenden, um das Modell zu trainieren."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d1b517b-b2c7-4c7a-9cd9-c7cff6c29607",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "ml.evaluation(X = sonar_df_mmtrans.values, y = sonar_df.values[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aa9a6b2-f660-4934-84b5-6b4609e47ac9",
   "metadata": {},
   "source": [
    "Die Accuarcy auf Basis der MinMaxScaler-Transformation sollte zu einer geringen Leistungssteigerung geführt haben.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa8533f2-8a19-41a0-8aca-c4f8be3b03d0",
   "metadata": {},
   "source": [
    "Im nächsten Schritt wenden wir den **StandardScaler** direkt auf den Sonar-Datensatz an, um die Inputvariablen zu standardisieren."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ea86512-6689-4b83-8a94-2e2ab181deb7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f6baf7f-be55-4146-b532-b6ae7d6f7d7f",
   "metadata": {},
   "source": [
    "**Aufgabe 13:** Führe eine Transformation mit Hilfe der `Standard Skalierung` durch. Führe anschließend den Test durch und sehe, wie sich die Performance des Klassifikators verändert. Sichere die transfomierten Inputvariablen in einen neuen DataFrame mit der Bezeichnung `sonar_df_sstrans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6f7664-7e0d-4cf4-a6bf-9d9e5d5a6ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a134796-d5c8-4b8c-8dac-d8382a88f641",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if all(sonar_df_sstrans.describe().loc['std'].values.round(decimals=0) == 1) and \\\n",
    "    all(sonar_df_sstrans.describe().loc['mean'].values.round(decimals=0) == 0) and \\\n",
    "        round(sonar_df_sstrans.sum().sum(), 1) == 0.0:\n",
    "    \n",
    "        print(\"Test successful\")\n",
    "        \n",
    "        print(sonar_df_sstrans.describe())\n",
    "        sonar_df.hist(bins=30, figsize=(15, 10))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf1d8d-91d7-48c1-bb2e-5f8735954156",
   "metadata": {},
   "source": [
    "Wir können sehen, dass die Verteilungen angepasst wurden und dass der Mittelwert eine sehr kleine Zahl nahe bei `0.0` ist und die Standardabweichung für jede Variable sehr nahe bei `1.0` liegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d4d73e-ef1f-4381-9e60-7b2497a52e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "ml.evaluation(X = sonar_df_sstrans.values, y = sonar_df.values[:, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275496e3-5143-4ff3-8476-f26ca6f1ac7e",
   "metadata": {},
   "source": [
    "Die Accuarcy auf Basis der StandardScaler-Transformation sollte zu einer geringen Leistungssteigerung geführt haben."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ce442c-e7a7-42f0-aef3-6d2555c48dff",
   "metadata": {},
   "source": [
    "### Interpretation der Ergebniss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f320e1-805b-4d34-9067-2123e716ae11",
   "metadata": {},
   "source": [
    "**Aufgabe 14:** Wann sollten Inputvariablen normalisiert (z. B. über `Min-Max-Scaler`) oder standardisiert (z. B. über `StandardScaler`) werden?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c99362-029c-4c00-bed8-2044d0557cad",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d348c1-4833-4166-a6e4-91bbed9f0d6b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee81fc-500a-4220-8652-bc89e3135931",
   "metadata": {},
   "source": [
    "**Aufgabe 15**: Sollte erst standardisiert und dann normalisiert werden?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21397886-c455-468d-840a-b4c2e3e85573",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23d639f3-c8d4-4821-83a1-b9e0142ad2fb",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ade2c3d9-b629-429f-bea6-22299d1137b9",
   "metadata": {},
   "source": [
    "**Aufgabe 16:** Was funktioniert nun besser? Standardisierung oder Normalisierung?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baea7f41-876e-4b3e-a447-bb8cdc5413cd",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58eb8f11-92f5-463f-845b-261175de62ed",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7bcde-71f9-4327-a730-e62d62eeb9ba",
   "metadata": {},
   "source": [
    "**Aufgabe 17:** Wie werden Werte behandelt, die potenziell später hinzukommen und so außerhalb der definierten Grenzen des Wertebereichs liegen?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb6fe17-105a-444a-9c4c-02e68d824294",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807d7ad-cbc4-4b16-bd2e-ddc6d3bcc697",
   "metadata": {},
   "source": [
    "![purple-divider](https://user-images.githubusercontent.com/7065401/52071927-c1cd7100-2562-11e9-908a-dde91ba14e59.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8dcf9-2db4-4812-9b42-7756fe8627f0",
   "metadata": {},
   "source": [
    "## 4. Impute von fehlenden Werten über K-Nearest Neighbor Algorithmus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7932f9ba-25a7-4203-9672-1358cdf147b8",
   "metadata": {},
   "source": [
    "Für die Durchführung der folgenden Übung verwenden wir wieder den **Boston Housing Datensatz** aus Abschnitt 2. Unten angestellt eine kleine Erinnerung wie der Datensatz aussieht:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec53c6f-041a-4f68-943b-699da7308893",
   "metadata": {},
   "outputs": [],
   "source": [
    "boston_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325db65d-ca8f-4c66-9937-5e662f9027d1",
   "metadata": {},
   "source": [
    "Der Datensatz hat keine fehlenden Werte bisher. Das werden wir kurz ändern. Anbei findet Ihr drei Index-Arrays, die zufällig gewälht sind mit den Größen `[40, 20, 5]`.\n",
    "\n",
    "Diese drei Index-Arrays verwenden wir, um die jeweiligen Werte für die Variablen `INDUS`, `TAX` und `RM` auf `NULL` zu setzen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59aa742a-131d-49dc-a9fe-4b5e8afbc2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "i1 = [25,  70, 204,  33, 348, 144,  39, 194, 140, 188, 225, 285, 418,\n",
    "      435, 352, 124, 481, 433, 192,  60, 436, 106, 468, 385, 498,  37,\n",
    "      160, 490, 289,  82,  14, 128, 114, 465,  37, 204, 342,  32, 266,\n",
    "      438]\n",
    "i2 = [154,  20, 418,  11, 439, 471, 383,  38,  27, 181, 163,  98, 266,\n",
    "      351, 100,  48, 292,  71, 502,   2]\n",
    "i3 = [89,  15,   5, 195, 403]\n",
    "\n",
    "boston_df.loc[i1, 'INDUS'] = np.nan\n",
    "boston_df.loc[i2, 'TAX'] = np.nan\n",
    "boston_df.loc[i3, 'RM'] = np.nan\n",
    "\n",
    "boston_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d40efa1-7ba5-46c7-aced-8cf6e73ad3c9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b12e664-b3c3-42ee-9c50-ef7c7b7d49d5",
   "metadata": {},
   "source": [
    "**Aufgabe 18**: Füre einen `KNN-Imputer`über `sklearn`aus. Importiere dafür die Library. Nutze für den KNN-Imputer den Parameter `n_neighbors=3`. Überführe die resultierenden Daten in einen gesamtheitlichen DataFrame mit dem Variablennamen `boston_df_imputed`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a7ab01-7546-478f-b808-aa397aae6133",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3724779a-74ca-4a6e-8f51-5f6a0583e267",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST HERE ###\n",
    "if boston_df_imputed.loc[i1, 'INDUS'].sum() + boston_df_imputed.loc[i2, 'TAX'].sum() + \\\n",
    "    boston_df_imputed.loc[i3, 'RM'].sum() == 8286.306333333332:\n",
    "    print(\"Test successful\\n\")\n",
    "    \n",
    "    print(\"Überprüfung fehlender Werte:\\n\", boston_df_imputed.isnull().sum())\n",
    "    \n",
    "    print(\"\\nImpute Werte für INDUS: \", list(boston_df_imputed.loc[i1, 'INDUS'].values))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28494cf-1b9e-4f7f-bb49-1cf434e3a5ad",
   "metadata": {},
   "source": [
    "In der vorangegangen Aufgabe haben wir den Parameter `n_neighbors=3` vorgegeben. Die Frage ist, wie wir den optimalen Wert für K (Anzahl der berücksichtigten nächsten Punkte) finden.\n",
    "\n",
    "Die ist stark abhängig von den später gewählten Modellen, ML-Algorithmen und Zielen der Analyse. Im nachfolgenden ist beispielhaft dargestellt, wie eine Optimierung für einen `Random Forest Algorithmus` aussieht.\n",
    "\n",
    "Die Optimierung ist eine **Zusatzaufgabe** für Frewillige.\n",
    "\n",
    "Zur Optimierung nehmen wir folgende Schritte an:\n",
    "- Iteriere über den möglichen Bereich für K - alle Zahlen zwischen 1 und 20\n",
    "- Durchführung der Imputation mit dem aktuellen K-Wert\n",
    "- Aufteilung des Datensatzes in eine Trainings- und eine Testgruppe\n",
    "- Anpassung des [Random-Forests-Modells](https://levelup.gitconnected.com/random-forest-regression-209c0f354c84)\n",
    "- Vorhersage für die Testmenge\n",
    "- Auswertung anhand des [RMSE](https://www.statisticshowto.com/probability-and-statistics/regression-analysis/rmse-root-mean-square-error/) (Root Mean Square Error)\n",
    "\n",
    "Folgende Funktionen können hier nachgelesen werden:\n",
    "- sklearn [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "- sklearn [`RandomForestRegressor`](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html)\n",
    "- sklearn [`mean_squared_error`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.mean_squared_error.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0499302a-9f79-4ecd-b485-ae968bb7780b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66171c2-f641-4b42-a589-625b67d0de69",
   "metadata": {},
   "source": [
    "**Aufgabe 19 (Zusatzaufgabe):**  Ergänze die fehlenden Zeilen auf Basis der Intruktionen. Versuche dem kommentierten Code zu folgen und die wesentlichen Schritte nachzuvollziehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf76f0c-4992-4ae3-b9da-e20f3b500733",
   "metadata": {},
   "outputs": [],
   "source": [
    "### CODE HERE ###\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from math import sqrt\n",
    "\n",
    "def optimize_k(data, target, range_k):\n",
    "    \n",
    "    # Initialisierung leerer Liste zum sichern der Ergebnisse für unterschiedliche K\n",
    "    errors = []\n",
    "    \n",
    "    # Loop über alle Werte im range_k\n",
    "    for k in range(1, range_k, 1):\n",
    "        \n",
    "        # TODO: Führe für Parameter k eine KNN-Imputation durch und sichere in DaraFrame df_imputed \n",
    "        \n",
    "        \n",
    "        # TODO: Teile die Daten in Inputvariable X und Targetvariable y - hier Variable target\n",
    "        \n",
    "        \n",
    "        # Teile die Daten in Trainings- und Testset über SKlearn train_test_split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        # Aufsetzen des Modells\n",
    "        model = RandomForestRegressor(random_state=42)\n",
    "        \n",
    "        # Trainieren des Modells\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Erhalt der Prediction für X_test\n",
    "        preds = model.predict(X_test)\n",
    "        \n",
    "        # Berechnung des Errors über RMSE auf Basis der y_test Daten und der Prediction preds\n",
    "        error = sqrt(mean_squared_error(y_test,preds))\n",
    "        \n",
    "        # Hinzufügen des Berechneten Errors zu Liste erros\n",
    "        errors.append({'K': k, 'RMSE': error})\n",
    "        \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75f277e-2bf7-4ca4-a148-a8b15a14376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aufrufen der Funktion\n",
    "k_errors = optimize_k(data=boston_df, target='PRICE', range_k=20)\n",
    "\n",
    "# Printe die Ergebnisse pro Iteration\n",
    "for line in k_errors:\n",
    "    print(line)\n",
    "    \n",
    "# Erhalte das Minimum des RMSE\n",
    "minPricedItem = min(k_errors, key=lambda x:x['RMSE'])\n",
    "print(\"\\nLowest Error Item: \", minPricedItem)\n",
    "\n",
    "# Visualisierung des RMSE gegen K\n",
    "curve = pd.DataFrame(data = k_errors)\n",
    "curve = curve.set_index('K')\n",
    "curve.plot()\n",
    "plt.ylabel('RMSE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e068dc0-af10-45a0-876b-cd7fe1873860",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d4c330e-74ae-4c72-ad4d-62f82510fc8d",
   "metadata": {},
   "source": [
    "**Aufgabe 20 (Zusatzaufgabe):**  Interpretiere die Ergebnisse. Was sagt der Wert von RMSE für K=12 aus? Und Spezialfrage: Warum ändert sich das Ergebnis, wenn der `RandomState` für das `RandomForestRegression` Model geändert wird?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc99d4f0-1403-4df2-b5c1-8026fae5c5ec",
   "metadata": {},
   "source": [
    "`### BEGRÜNDUNG HERE ###`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
